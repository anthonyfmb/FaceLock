{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeIdentifierFile(path):\n",
    "    for file in os.listdir(path):\n",
    "        if file.find(\".Identifier\") != -1:\n",
    "            os.remove(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "raw_imposters_folder = \"imposters\"\n",
    "raw_valid_user_folder = \"valid_user\"\n",
    "\n",
    "removeIdentifierFile(raw_imposters_folder)\n",
    "removeIdentifierFile(raw_valid_user_folder)\n",
    "\n",
    "img_count = 500 # img taken from each class\n",
    "\n",
    "imposter_filenames = random.sample(os.listdir(raw_imposters_folder), img_count)\n",
    "valid_user_filenames = random.sample(os.listdir(raw_valid_user_folder), img_count)\n",
    "\n",
    "train_valid_user_folder = \"train/valid_user\"\n",
    "train_imposters_folder = \"train/imposters\"\n",
    "\n",
    "Path(train_valid_user_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(train_imposters_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_valid_user_folder = \"test/valid_user\"\n",
    "test_imposters_folder = \"test/imposters\"\n",
    "\n",
    "Path(test_valid_user_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_imposters_folder).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "class_training_size = int(img_count * 0.9)\n",
    "\n",
    "count = 0\n",
    "for imposter_filename in imposter_filenames[:class_training_size]:\n",
    "    shutil.copy(os.path.join(raw_imposters_folder, imposter_filename), os.path.join(train_imposters_folder, imposter_filename))\n",
    "    count += 1\n",
    "\n",
    "count = 0\n",
    "for imposter_filename in imposter_filenames[class_training_size:]:\n",
    "    shutil.copy(os.path.join(raw_imposters_folder, imposter_filename), os.path.join(test_imposters_folder, imposter_filename))\n",
    "\n",
    "\n",
    "for valid_user_filename in valid_user_filenames[:class_training_size]:\n",
    "    shutil.copy(os.path.join(raw_valid_user_folder, valid_user_filename), os.path.join(train_valid_user_folder, valid_user_filename))\n",
    "\n",
    "for valid_user_filename in valid_user_filenames[class_training_size:]:\n",
    "    shutil.copy(os.path.join(raw_valid_user_folder, valid_user_filename), os.path.join(test_valid_user_folder, valid_user_filename))\n",
    "\n",
    "removeIdentifierFile(train_imposters_folder)\n",
    "removeIdentifierFile(test_imposters_folder)\n",
    "removeIdentifierFile(train_valid_user_folder)\n",
    "removeIdentifierFile(test_valid_user_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 2 classes.\n",
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255)\n",
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_dataset = train.flow_from_directory(\"train\",\n",
    "    target_size=(150,150),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary')\n",
    "                                         \n",
    "test_dataset = test.flow_from_directory(\"test\",\n",
    "    target_size=(150,150),\n",
    "    batch_size =32,\n",
    "    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imposters': 0, 'valid_user': 1}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.class_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 12:41:52.578144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-21 12:41:52.578836: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/magician3124/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-21 12:41:52.579707: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/magician3124/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-21 12:41:52.579811: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/magician3124/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-21 12:41:52.579893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/magician3124/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-21 12:41:52.580057: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/magician3124/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-21 12:41:52.580214: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/magician3124/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-21 12:41:52.580321: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/magician3124/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-21 12:41:52.580730: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/magician3124/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-21 12:41:52.580776: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-21 12:41:52.589217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9941/1593974248.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_dataset,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 29/250 [==>...........................] - ETA: 1:48 - loss: 0.0229 - accuracy: 0.9967WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2500 batches). You may need to use the repeat() function when building your dataset.\n",
      "250/250 [==============================] - 16s 60ms/step - loss: 0.0229 - accuracy: 0.9967 - val_loss: 0.0035 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier/assets\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_dataset,\n",
    "    steps_per_epoch = 250,\n",
    "    epochs = 10,\n",
    "    validation_data = test_dataset\n",
    ")\n",
    "\n",
    "model.save('classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
