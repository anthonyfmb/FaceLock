{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeIdentifierFile(path):\n",
    "    for file in os.listdir(path):\n",
    "        if file.find(\".Identifier\") != -1:\n",
    "            os.remove(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "raw_imposters_folder = \"new_imposter\"\n",
    "raw_valid_user_folder = \"valid_user\"\n",
    "\n",
    "removeIdentifierFile(raw_imposters_folder)\n",
    "removeIdentifierFile(raw_valid_user_folder)\n",
    "\n",
    "img_count = 500 # img taken from each class\n",
    "\n",
    "imposter_filenames = random.sample(os.listdir(raw_imposters_folder), img_count)\n",
    "valid_user_filenames = random.sample(os.listdir(raw_valid_user_folder), img_count)\n",
    "\n",
    "train_valid_user_folder = \"train/valid_user\"\n",
    "train_imposters_folder = \"train/imposters\"\n",
    "\n",
    "Path(train_valid_user_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(train_imposters_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_valid_user_folder = \"test/valid_user\"\n",
    "test_imposters_folder = \"test/imposters\"\n",
    "\n",
    "Path(test_valid_user_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_imposters_folder).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_training_size = int(img_count * 0.9)\n",
    "\n",
    "count = 0\n",
    "for imposter_filename in imposter_filenames[:class_training_size]:\n",
    "    shutil.copy(os.path.join(raw_imposters_folder, imposter_filename), os.path.join(train_imposters_folder, imposter_filename))\n",
    "    count += 1\n",
    "\n",
    "count = 0\n",
    "for imposter_filename in imposter_filenames[class_training_size:]:\n",
    "    shutil.copy(os.path.join(raw_imposters_folder, imposter_filename), os.path.join(test_imposters_folder, imposter_filename))\n",
    "\n",
    "\n",
    "for valid_user_filename in valid_user_filenames[:class_training_size]:\n",
    "    shutil.copy(os.path.join(raw_valid_user_folder, valid_user_filename), os.path.join(train_valid_user_folder, valid_user_filename))\n",
    "\n",
    "for valid_user_filename in valid_user_filenames[class_training_size:]:\n",
    "    shutil.copy(os.path.join(raw_valid_user_folder, valid_user_filename), os.path.join(test_valid_user_folder, valid_user_filename))\n",
    "\n",
    "removeIdentifierFile(train_imposters_folder)\n",
    "removeIdentifierFile(test_imposters_folder)\n",
    "removeIdentifierFile(train_valid_user_folder)\n",
    "removeIdentifierFile(test_valid_user_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 2 classes.\n",
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255)\n",
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_dataset = train.flow_from_directory(\"train\",\n",
    "    target_size=(150,150),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary')\n",
    "                                         \n",
    "test_dataset = test.flow_from_directory(\"test\",\n",
    "    target_size=(150,150),\n",
    "    batch_size =32,\n",
    "    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imposters': 0, 'valid_user': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.class_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16433/1593974248.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_dataset,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29/250 [==>...........................] - ETA: 1:16 - loss: 0.1711 - accuracy: 0.9389WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2500 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2500 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 13s 41ms/step - loss: 0.1711 - accuracy: 0.9389 - val_loss: 3.6751e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier/assets\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_dataset,\n",
    "    steps_per_epoch = 250,\n",
    "    epochs = 10,\n",
    "    validation_data = test_dataset\n",
    ")\n",
    "\n",
    "model.save('classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImage(filename):\n",
    "    img1 = image.load_img(filename,target_size=(150,150))\n",
    "    model = tf.keras.models.load_model('classifier')\n",
    " \n",
    "    Y = image.img_to_array(img1)\n",
    "    \n",
    "    X = np.expand_dims(Y,axis=0)\n",
    "    val = model.predict(X, verbose=0)\n",
    "    return val[0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.png correct 0.0\n",
      "84.png correct 0.0\n",
      "209.png correct 0.0\n",
      "366.png correct 0.0\n",
      "524.png correct 0.0\n",
      "490.png correct 0.0\n",
      "302.png correct 0.0\n",
      "129.png correct 0.0\n",
      "575.png correct 0.0\n",
      "350.png correct 0.0\n",
      "76.png correct 0.0\n",
      "30.png correct 0.0\n",
      "298.png correct 0.0\n",
      "281.png correct 0.0\n",
      "597.png correct 0.0\n",
      "372.png correct 0.0\n",
      "109.png correct 0.0\n",
      "125.png correct 0.0\n",
      "244.png correct 0.0\n",
      "478.png correct 0.0\n",
      "530.png correct 0.0\n",
      "483.png correct 0.0\n",
      "54.png correct 0.0\n",
      "14.png correct 0.0\n",
      "67.png correct 0.0\n",
      "568.png correct 0.0\n",
      "441.png correct 0.0\n",
      "592.png correct 0.0\n",
      "294.png correct 0.0\n",
      "376.png correct 0.0\n",
      "361.png correct 0.0\n",
      "454.png correct 0.0\n",
      "534.png correct 0.0\n",
      "37.png correct 0.0\n",
      "521.png correct 0.0\n",
      "26.png correct 0.0\n",
      "64.png correct 0.0\n",
      "117.png correct 0.0\n",
      "507.png correct 0.0\n",
      "137.png correct 0.0\n",
      "90.png correct 0.0\n",
      "313.png correct 0.0\n",
      "532.png correct 0.0\n",
      "23.png correct 0.0\n",
      "49.png correct 0.0\n",
      "408.png correct 0.0\n",
      "257.png correct 0.0\n",
      "216.png correct 0.0\n",
      "519.png correct 0.0\n",
      "509.png correct 0.0\n",
      "580.png correct 1.0\n",
      "60.png correct 1.0\n",
      "562.png correct 1.0\n",
      "353.png correct 1.0\n",
      "105.png correct 1.0\n",
      "214.png correct 1.0\n",
      "409.png correct 1.0\n",
      "527.png correct 1.0\n",
      "246.png correct 1.0\n",
      "241.png correct 1.0\n",
      "269.png correct 1.0\n",
      "430.png correct 1.0\n",
      "302.png correct 1.0\n",
      "30.png correct 1.0\n",
      "192.png correct 1.0\n",
      "339.png correct 1.0\n",
      "277.png correct 1.0\n",
      "579.png correct 1.0\n",
      "583.png correct 1.0\n",
      "268.png correct 1.0\n",
      "281.png correct 1.0\n",
      "321.png correct 1.0\n",
      "304.png correct 1.0\n",
      "10.png correct 1.0\n",
      "540.png correct 1.0\n",
      "85.png correct 1.0\n",
      "100.png correct 1.0\n",
      "202.png correct 1.0\n",
      "357.png correct 1.0\n",
      "171.png correct 1.0\n",
      "434.png correct 1.0\n",
      "383.png correct 1.0\n",
      "111.png correct 1.0\n",
      "309.png correct 1.0\n",
      "69.png correct 1.0\n",
      "114.png correct 1.0\n",
      "267.png correct 1.0\n",
      "436.png correct 1.0\n",
      "23.png correct 1.0\n",
      "49.png correct 1.0\n",
      "567.png correct 1.0\n",
      "319.png correct 1.0\n",
      "557.png correct 1.0\n",
      "546.png correct 1.0\n",
      "252.png correct 1.0\n",
      "72.png correct 1.0\n",
      "349.png correct 1.0\n",
      "450.png correct 1.0\n",
      "467.png correct 1.0\n",
      "190.png correct 1.0\n"
     ]
    }
   ],
   "source": [
    "test_imposter = \"test/imposters\"\n",
    "test_valid_user = \"test/valid_user\"\n",
    "correct_count = 0\n",
    "wrong_count = 0\n",
    "count = 0\n",
    "\n",
    "for file in os.listdir(test_imposter):\n",
    "    id = predictImage(os.path.join(test_imposter, file))\n",
    "    count += 1\n",
    "    if id == 0:\n",
    "        correct_count += 1\n",
    "        print(f\"{file} correct {id}\")\n",
    "    else:\n",
    "        wrong_count += 1\n",
    "        print(f\"{file} wrong {id}\")\n",
    "\n",
    "\n",
    "for file in os.listdir(test_valid_user):\n",
    "    id = predictImage(os.path.join(test_valid_user, file))\n",
    "    count += 1\n",
    "    if id == 1:\n",
    "        correct_count += 1\n",
    "        print(f\"{file} correct {id}\")\n",
    "    else:\n",
    "        wrong_count += 1\n",
    "        print(f\"{file} wrong {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(correct_count / count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 10:18:28) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
